


# my_main.py
from dotenv import load_dotenv
from agents.core.llm_client import LlmClient    # æ³¨æ„:è¿™é‡Œå¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„ç±»
 
from agents.agent.simple_agent import SimpleAgent
from agents.tools.registry import ToolRegistry
from agents.tools.async_executor import AsyncToolExecutor
from agents.tools.builtin.calculator import CalculatorTool
from agents.agent.react_agent import ReActAgent
from agents.agent.reflection_agent import ReflectionAgent
# from agents import create_calculator_registry

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()



def test_swiatch_provider():
    
    # å®ä¾‹åŒ–æˆ‘ä»¬é‡å†™çš„å®¢æˆ·ç«¯ï¼Œå¹¶æŒ‡å®šprovider
    llm = LlmClient(provider="llama.cpp") 
    # llm = LLM(provider="modelscope1") # ä¹Ÿå¯ä»¥è¿™æ ·æŒ‡å®š

    # å‡†å¤‡æ¶ˆæ¯
    messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]

    # å‘èµ·è°ƒç”¨ï¼Œthinkç­‰æ–¹æ³•éƒ½å·²ä»çˆ¶ç±»ç»§æ‰¿ï¼Œæ— éœ€é‡å†™
    response_stream = llm.think(messages)

    # æ‰“å°å“åº”
    print("ModelScope Response:")
    for chunk in response_stream:
        # chunkåœ¨my_llmåº“ä¸­å·²ç»æ‰“å°è¿‡ä¸€éï¼Œè¿™é‡Œåªéœ€è¦passå³å¯
        print(chunk, end="", flush=True)
        pass



# def test_calculator_tool():
#     """æµ‹è¯•è‡ªå®šä¹‰è®¡ç®—å™¨å·¥å…·"""

#     # åˆ›å»ºåŒ…å«è®¡ç®—å™¨çš„æ³¨å†Œè¡¨
#     registry = create_calculator_registry()

#     print("ğŸ§ª æµ‹è¯•è‡ªå®šä¹‰è®¡ç®—å™¨å·¥å…·\n")

#     # ç®€å•æµ‹è¯•ç”¨ä¾‹
#     test_cases = [
#         "2 + 3",           # åŸºæœ¬åŠ æ³•
#         "10 - 4",          # åŸºæœ¬å‡æ³•
#         "5 * 6",           # åŸºæœ¬ä¹˜æ³•
#         "15 / 3",          # åŸºæœ¬é™¤æ³•
#         "sqrt(16)",        # å¹³æ–¹æ ¹
#     ]

#     for i, expression in enumerate(test_cases, 1):
#         print(f"æµ‹è¯• {i}: {expression}")
#         result = registry.execute_tool("my_calculator", expression)
#         print(f"ç»“æœ: {result}\n")

# def test_with_simple_agent():
#     """æµ‹è¯•ä¸SimpleAgentçš„é›†æˆ"""
#     from agents import AgentsLLM

#     # åˆ›å»ºLLMå®¢æˆ·ç«¯
#     llm = AgentsLLM()

#     # åˆ›å»ºåŒ…å«è®¡ç®—å™¨çš„æ³¨å†Œè¡¨
#     registry = create_calculator_registry()

#     print("ğŸ¤– ä¸SimpleAgenté›†æˆæµ‹è¯•:")

#     # æ¨¡æ‹ŸSimpleAgentä½¿ç”¨å·¥å…·çš„åœºæ™¯
#     user_question = "è¯·å¸®æˆ‘è®¡ç®— sqrt(16) + 2 * 3"

#     print(f"ç”¨æˆ·é—®é¢˜: {user_question}")

#     # ä½¿ç”¨å·¥å…·è®¡ç®—
#     calc_result = registry.execute_tool("my_calculator", "sqrt(16) + 2 * 3")
#     print(f"è®¡ç®—ç»“æœ: {calc_result}")

#     # æ„å»ºæœ€ç»ˆå›ç­”
#     final_messages = [
#         {"role": "user", "content": f"è®¡ç®—ç»“æœæ˜¯ {calc_result}ï¼Œè¯·ç”¨è‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜:{user_question}"}
#     ]

#     print("\nğŸ¯ SimpleAgentçš„å›ç­”:")
#     response = llm.think(final_messages)
#     for chunk in response:
#         print(chunk, end="", flush=True)
#     print("\n")

# if __name__ == "__main__":
#     test_calculator_tool()
#     test_with_simple_agent()







def test_simaple_agent():
    # åˆ›å»ºLLMå®ä¾‹
    llm = LlmClient(provider="llama.cpp")

    # æµ‹è¯•1:åŸºç¡€å¯¹è¯Agentï¼ˆæ— å·¥å…·ï¼‰
    print("=== æµ‹è¯•1:åŸºç¡€å¯¹è¯ ===")
    basic_agent = SimpleAgent(
        name="åŸºç¡€åŠ©æ‰‹",
        llm=llm,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„æ–¹å¼å›ç­”é—®é¢˜ã€‚"
    )

    response1 = basic_agent.run("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
    print(f"åŸºç¡€å¯¹è¯å“åº”: {response1}\n")

    # æµ‹è¯•2:å¸¦å·¥å…·çš„Agent
    print("=== æµ‹è¯•2:å·¥å…·å¢å¼ºå¯¹è¯ ===")
    tool_registry = ToolRegistry()
    calculator = CalculatorTool()
    tool_registry.register_tool(calculator)

    enhanced_agent = SimpleAgent(
        name="å¢å¼ºåŠ©æ‰‹",
        llm=llm,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚",
        tool_registry=tool_registry,
        enable_tool_calling=True
    )

    response2 = enhanced_agent.run("è¯·å¸®æˆ‘è®¡ç®— 15 * 8 + 32")
    print(f"å·¥å…·å¢å¼ºå“åº”: {response2}\n")

    # æµ‹è¯•3:æµå¼å“åº”
    print("=== æµ‹è¯•3:æµå¼å“åº” ===")
    print("æµå¼å“åº”: ", end="")
    chunk_data = "";
    for chunk in basic_agent.stream_run("è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"):
        # print(f"{chunk}");
        #
        #pass  # å†…å®¹å·²åœ¨stream_runä¸­å®æ—¶æ‰“å°
        pass

    # æµ‹è¯•4:åŠ¨æ€æ·»åŠ å·¥å…·
    print("\n=== æµ‹è¯•4:åŠ¨æ€å·¥å…·ç®¡ç† ===")
    print(f"æ·»åŠ å·¥å…·å‰: {basic_agent.has_tools()}")
    basic_agent.add_tool(calculator)
    print(f"æ·»åŠ å·¥å…·å: {basic_agent.has_tools()}")
    print(f"å¯ç”¨å·¥å…·: {basic_agent.list_tools()}")

    # æŸ¥çœ‹å¯¹è¯å†å²
    print(f"\nå¯¹è¯å†å²: {len(basic_agent.get_history())} æ¡æ¶ˆæ¯")




def test_reflection_agent():
    llm = LlmClient(provider="llama.cpp");

    # ä½¿ç”¨é»˜è®¤é€šç”¨æç¤ºè¯
    general_agent = ReflectionAgent(name="æˆ‘çš„åæ€åŠ©æ‰‹", llm=llm);
    
    # ä½¿ç”¨è‡ªå®šä¹‰ä»£ç ç”Ÿæˆæç¤ºè¯(ç±»ä¼¼ç¬¬å››ç« )
    code_prompts = {
        "initial": "ä½ æ˜¯Pythonä¸“å®¶ï¼Œ è¯·ç¼–å†™å‡½æ•°:{task}",
        "reflect": "è¯·å®¡æŸ¥ä»£ç çš„ç®—æ³•æ•ˆç‡:\nä»»åŠ¡:{task}\nä»£ç :{content}",
        "refine": "è¯·æ ¹æ®åé¦ˆä¼˜åŒ–ä»£ç :\nä»»åŠ¡:{task}\nåé¦ˆ:{feedback}",
    }

    code_agent = ReflectionAgent(name="æˆ‘çš„ä»£ç ç”ŸæˆåŠ©æ‰‹",
                                 llm=llm,
                                 custom_prompts=code_prompts);

    # æµ‹è¯•ä½¿ç”¨
    result = general_agent.run("å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½å‘å±•å†ç¨‹çš„ç®€çŸ­æ–‡ç« ")
    print(f"æœ€ç»ˆç»“æœ:{result}");

if __name__ == "__main__":
    #test_swiatch_provider()
    # test_simaple_agent()
    test_reflection_agent()