{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214847fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "class HelloAgentsLLM:\n",
    "    \"\"\"\n",
    "    ä¸ºæœ¬ä¹¦ \"Hello Agents\" å®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚\n",
    "    å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚\n",
    "        \"\"\"\n",
    "        self.model = model or os.getenv(\"LLM_MODEL_ID\")\n",
    "        apiKey = apiKey or os.getenv(\"LLM_API_KEY\")\n",
    "        baseUrl = baseUrl or os.getenv(\"LLM_BASE_URL\")\n",
    "        timeout = timeout or int(os.getenv(\"LLM_TIMEOUT\", 60))\n",
    "        \n",
    "        if not all([self.model, apiKey, baseUrl]):\n",
    "            raise ValueError(\"æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚\")\n",
    "\n",
    "        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)\n",
    "\n",
    "    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:\n",
    "        \"\"\"\n",
    "        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...\")\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            # å¤„ç†æµå¼å“åº”\n",
    "            print(\"âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\")\n",
    "            collected_content = []\n",
    "            for chunk in response:\n",
    "                content = chunk.choices[0].delta.content or \"\"\n",
    "                print(content, end=\"\", flush=True)\n",
    "                collected_content.append(content)\n",
    "            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ\n",
    "            return \"\".join(collected_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "            return None\n",
    "\n",
    "# --- å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹ ---\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        llmClient = HelloAgentsLLM()\n",
    "        \n",
    "        exampleMessages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes Python code.\"},\n",
    "            {\"role\": \"user\", \"content\": \"å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•\"}\n",
    "        ]\n",
    "        \n",
    "        print(\"--- è°ƒç”¨LLM ---\")\n",
    "        responseText = llmClient.think(exampleMessages)\n",
    "        if responseText:\n",
    "            print(\"\\n\\n--- å®Œæ•´æ¨¡å‹å“åº” ---\")\n",
    "            print(responseText)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# >>>\n",
    "# --- è°ƒç”¨LLM ---\n",
    "# ğŸ§  æ­£åœ¨è°ƒç”¨ xxxxxx æ¨¡å‹...\n",
    "# âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
    "# å¿«é€Ÿæ’åºæ˜¯ä¸€ç§éå¸¸é«˜æ•ˆçš„æ’åºç®—æ³•...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claude_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
